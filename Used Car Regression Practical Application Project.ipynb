{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What drives the price of a car?\n",
    "\n",
    "![](images/kurt.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OVERVIEW**\n",
    "\n",
    "In this application, you will explore a dataset from Kaggle. The original dataset contained information on 3 million used cars. The provided dataset contains information on 426K cars to ensure speed of processing.  Your goal is to understand what factors make a car more or less expensive.  As a result of your analysis, you should provide clear recommendations to your client -- a used car dealership -- as to what consumers value in a used car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CRISP-DM Framework\n",
    "\n",
    "<center>\n",
    "    <img src = images/crisp.png width = 50%/>\n",
    "</center>\n",
    "\n",
    "\n",
    "To frame the task, throughout our practical applications, we will refer back to a standard process in industry for data projects called CRISP-DM.  This process provides a framework for working through a data problem.  Your first step in this application will be to read through a brief overview of CRISP-DM [here](https://mo-pcco.s3.us-east-1.amazonaws.com/BH-PCMLAI/module_11/readings_starter.zip).  After reading the overview, answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "From a business perspective, we are tasked with identifying key drivers for used car prices.  In the CRISP-DM overview, we are asked to convert this business framing to a data problem definition.  Using a few sentences, reframe the task as a data task with the appropriate technical vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background:  \n",
    "The relationship between the price of a used car and single variables like the antiquity of the model or the visible condition of the vehicle is relatively intuitive (i.e. greater antiquity decreases price, better visible condition increases price), although when these variables are combined the actual relationship is more difficult to characterize. For this purpose, we will design and implement a machine learning regression model that combines separate characteristics (variables) in a manner that allows us to derive more exact conclusions regarding how to optimize the pricing system for a lot filled with used cars.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:  \n",
    "Derive deeper understanding regarding consumer preferences for purchasing used cars from regression model to optimize inventory and sales strategies for used car salesman and used car lot owners**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "After considering the business understanding, we want to get familiar with our data.  Write down some steps that you would take to get to know the dataset and identify any quality issues within.  Take time to get to know the dataset and explore what information it contains and how this could be used to inform your business understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary packages, objects, and functions\n",
    "from sklearn.pipeline import Pipeline\n",
    "from category_encoders import MEstimateEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler \n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn import set_config\n",
    "from zipfile import ZipFile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "set_config(display=\"diagram\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display parameters for all printed pandas dataframe tables\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# freq_encoder was used in testing the model but was determined to not be the optimal metric\n",
    "def freq_encoder(df,col_name):\n",
    "    freq_dict = {}\n",
    "    uniqs = df[col_name].unique()\n",
    "    for cat in uniqs:\n",
    "        freq_dict[cat] = df[col_name].loc[df[col_name] == cat].value_counts()[0]\n",
    "    df[col_name].replace(freq_dict,inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Initial Checks**\n",
    "- Check the data types of each variable (int, object, str, etc.)\n",
    "- Check the amount of null values for each variable\n",
    "- Check the amount of unique values for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 426880 entries, 0 to 426879\n",
      "Data columns (total 18 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   id            426880 non-null  int64  \n",
      " 1   region        426880 non-null  object \n",
      " 2   price         426880 non-null  int64  \n",
      " 3   year          425675 non-null  float64\n",
      " 4   manufacturer  409234 non-null  object \n",
      " 5   model         421603 non-null  object \n",
      " 6   condition     252776 non-null  object \n",
      " 7   cylinders     249202 non-null  object \n",
      " 8   fuel          423867 non-null  object \n",
      " 9   odometer      422480 non-null  float64\n",
      " 10  title_status  418638 non-null  object \n",
      " 11  transmission  424324 non-null  object \n",
      " 12  VIN           265838 non-null  object \n",
      " 13  drive         296313 non-null  object \n",
      " 14  size          120519 non-null  object \n",
      " 15  type          334022 non-null  object \n",
      " 16  paint_color   296677 non-null  object \n",
      " 17  state         426880 non-null  object \n",
      "dtypes: float64(2), int64(2), object(14)\n",
      "memory usage: 58.6+ MB\n"
     ]
    }
   ],
   "source": [
    "zip_file = ZipFile('data/vehicles.zip')\n",
    "cars = pd.read_csv(zip_file.open('vehicles.csv'))\n",
    "# cars = pd.read_csv('data/vehicles.zip', compression = 'zip')\n",
    "cars.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = cars.columns\n",
    "uniq_cnt = []\n",
    "null_cnt = []\n",
    "for column in column_names:\n",
    "    uniq_cnt.append(len(cars[column].unique()))\n",
    "    null_cnt.append(cars[column].isnull().sum())\n",
    "\n",
    "variable_info_df = pd.DataFrame(data = zip(null_cnt,uniq_cnt), index = column_names, columns = ['n_null_values','n_uniques']).sort_values('n_null_values',ascending=False)\n",
    "variable_info_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Conclusions from variable inspection**\n",
    "- Variables that can identify each individual car (ID, VIN)\n",
    "- Variables detailing the location of car sale (region, state)\n",
    "- Variables indicating external characteristics of cars (paint color, size, type)\n",
    "- Variables describing the ignition/operation of car (cylinders, fuel, transmission, drive)\n",
    "- Variables for production characteristics (manufacturer, model)\n",
    "- Variables for the state of car being sold (condition, title status, odometer, year)\n",
    "- TARGET VARIABLE: price\n",
    "\n",
    "**Removing vehicle identifiers is the first step as well as variables that could contain repetitive information**\n",
    "- VIN and ID can be eliminated for our purposes\n",
    "- Region and state are redundant, and since region has more unique values we will eliminate it and keep state\n",
    "- Size, condition, drive and paint color will be eliminated due to massive amounts of missing values\n",
    "- Cylinders could reflect the quantity of wear on the engine, so I suspect this variable will be useful\n",
    "- If we are going to use cylinders, we can transform the text to numerical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **We will now check the distributions of our numerical values: price, odometer, and year**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will start by using plotly.express for speed of generating the histograms\n",
    "px.histogram(cars['price'],\n",
    "            labels = {'value': 'Price ($)'},\n",
    "            title = 'Distribution of Prices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(cars['odometer'],\n",
    "             labels = {'value': 'Distance traveled (miles)'},\n",
    "             title = 'Distribution of Odometer Values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(cars['year'],\n",
    "            labels = {'value': 'Year'},\n",
    "            title = 'Years of Production')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From these histograms, we can draw various conclusions:**\n",
    "- There are a great deal of outliers for price, odometer, and year\n",
    "- The values for year should be changed to age values for our regression purposes\n",
    "- We will test taking the logarithm of price and odometer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) subplot hist for all numerical variables\n",
    "2) subplot hist for all categorical variables that are eliminated\n",
    "3) subplot hist for all categorical variables that are included\n",
    "4) subplot hist after cleaning\n",
    "5) subplot hist after taking log values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make plots using seaborn\n",
    "    - scatter of odometer vs. price\n",
    "    - scatter of log odometer vs. log price (painted for various categorical variables\n",
    "    - histplots with density function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "After our initial exploration and fine-tuning of the business understanding, it is time to construct our final dataset prior to modeling.  Here, we want to make sure to handle any integrity issues and cleaning, the engineering of new features, any transformations that we believe should happen (scaling, logarithms, normalization, etc.), and general preparation for modeling with `sklearn`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We use visual inspection of distributions for price as an example for identifying our outlier bounds, and the same process is applied to odometer and year**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(cars['price'].loc[(cars['price'] < 500000)]).set_title('Distribution of Car Prices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visually assess that the upper bound needs to be decreased, and that a lower bound should be introduced as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(cars['price'].loc[(cars['price'] > 1000) & (cars['price'] < 60000)]).set_title('Distribution of Car Prices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also try taking the logarithm of these values, which makes the distribution closer to a Gaussian (respecting the assumption of normality for linear regressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(cars['price'].loc[(cars['price'] > 1000) & (cars['price'] < 60000)], log_scale=True).set_title('Distribution of Car Prices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will check the same results for odometer\n",
    "sns.histplot(cars['odometer'].loc[(cars['odometer'] > 1000) & (cars['odometer'] < 400000)], log_scale=True).set_title('Distribution of Odometer Readings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check the year values as well\n",
    "cars['year'] = 2022-cars['year']\n",
    "sns.histplot(cars['year'].loc[(cars['year'] < 40)]).set_title('Distribution of Years of Production')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Having identified reasonable ranges and transformations for our numerical variables, we will assess our categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepared data with appropriate ranges for price, odometer, and year\n",
    "cars_fix = cars.loc[(cars['price'] < 58000) & (cars['price'] > 1000) & (cars['odometer'] < 400000) & (cars['odometer'] > 10000)]\n",
    "cars_fix = cars_fix.loc[(cars['year'] < 40)]\n",
    "\n",
    "#Transform price and odometer values with log10\n",
    "cars_fix['log_price'] = cars_fix['price'].apply(lambda x: np.log10(x) if x!= 0 else 0)\n",
    "cars_fix['log_odometer'] = cars_fix['odometer'].apply(lambda x: np.log10(x) if x!= 0 else 0)\n",
    "cars_fix.drop(['odometer','price'],axis = 1, inplace=True)\n",
    "\n",
    "cars_fix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "From this point on, we will use log_price and log_odometer to replace price and odometer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can see the boxplots of our categorical variables related to the log_price variable\n",
    "#    The important thing to note is that the ranges covered by the boxes has a large amount of overlap, which will support our regression\n",
    "#    The impact of the observed outliers will be minimized by our encoding scheme\n",
    "fig, axes = plt.subplots(ncols=4, nrows=1, sharey = True, figsize=(15,6))\n",
    "sns.boxplot(data = cars_fix, x = 'fuel', y = 'log_price', ax = axes[0]).set_title('Fuel Boxplot')\n",
    "axes[0].tick_params(axis='x', labelrotation=90)\n",
    "sns.boxplot(data = cars_fix, x = 'transmission', y = 'log_price', ax = axes[1]).set_title('Transmission Boxplot')\n",
    "axes[1].tick_params(axis='x', labelrotation=90)\n",
    "sns.boxplot(data = cars_fix, x = 'title_status', y = 'log_price', ax = axes[2]).set_title('Title Status Boxplot')\n",
    "axes[2].tick_params(axis='x', labelrotation=90)\n",
    "sns.boxplot(data = cars_fix, x = 'type', y = 'log_price', ax = axes[3]).set_title('Car Type Boxplot')\n",
    "axes[3].tick_params(axis='x', labelrotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will visualize the distributions for our remaining categorical variables: manufacturer, state\n",
    "- There are too many unique values for models to visualize\n",
    "\n",
    "**These variables, as well as the previous categorical variables, will be encoded based on their corresponding prices, which balances out the lack of instances for individual categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "sns.histplot(data = cars_fix, x = 'manufacturer').set_title('Distribution of Manufacturers')\n",
    "_ = plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "sns.histplot(data = cars_fix, x = 'state').set_title('Distribution of States')\n",
    "_ = plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will prepare our final variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Translate cylinders into numerical values and replace in cars_fix dataframe\n",
    "cylinder_rename = {'3 cylinders': 3, '4 cylinders': 4, '5 cylinders': 5, '6 cylinders': 6,\n",
    "                   '8 cylinders': 8, '10 cylinders': 10, '12 cylinders': 12, 'other': 16}\n",
    "cars_fix['cylinders'].replace(cylinder_rename, inplace=True)\n",
    "\n",
    "#Drop other unnecessary variables\n",
    "cars_fix = cars_fix.drop(['VIN','id','size','paint_color','region','condition','drive'], axis = 1).dropna() #'id','VIN','condition','cylinders','size','drive','paint_color','region'\n",
    "\n",
    "cars_fix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have managed to preserve ~40% of our data after all of our data cleaning and variable inclusions/exclusions**\n",
    "- 170k data points is a robust dataset that permits a high level of statistical power, sufficient for our current use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "With your (almost?) final dataset in hand, it is now time to build some models.  Here, you should build a number of different regression models with the price as the target.  In building your models, you should explore different parameters and be sure to cross-validate your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will try 3 types of regression:**\n",
    "- Ridge regression\n",
    "- Lasso regression\n",
    "- Regression with polynomial features\n",
    "\n",
    "**We will use 2 different transformers in our pipeline:**\n",
    "- MEstimateEncoder: transforms categorical values into their corresponding target values\n",
    "- PolynomialFeatures: to combine the influence of log_odometer, year, and cylinders since these variables should have some nonlinear effects on our data\n",
    "\n",
    "**We will use MSE, MAE, and R2 score as the evaluation metrics for our cross-validation and the strength of our model**\n",
    "- The combination of MSE and MAE allows us to identify undo influence from outliers\n",
    "- Since we are dealing with a numerical regression instead of a classification problem, the most appropriate metric for comparing our model to the underlying dataset would be the R2 score. This allows us assess how close our model is to the underlying dataset based on our target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***It is important to note that the results of our models will also be assessed using the coefficients derived from each regression as well as the permutation importance that evaluates the influence of each variable on our target value (log_price)***\n",
    "- The permutation importance will give us a measure of the influence that each of our used car characteristics had on the result\n",
    "- The coefficients will give us a measure of the degree of influence that each regressor had on the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's begin by declaring our output table for the evaluation of our three different regression models\n",
    "#  We have the train and test values for comparison of the MSE, MAE, and R2 score values\n",
    "#  We also have the optimum alpha value determined by our GridSearchCV for the two models that accept this parameter (ridge and lasso)\n",
    "model_results = pd.DataFrame(columns=['loss','Linear','Ridge','Lasso'])\n",
    "model_results['loss']=['alpha', 'MSE_Train','MSE_Test','MAE_Train','MAE_Test','R2_Train','R2_Test']\n",
    "model_results = model_results.set_index('loss')\n",
    "model_results.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we declare our transformer pipeline and our cross-validation datasets\n",
    "degree = 4\n",
    "targcats = ['manufacturer','transmission','fuel','state','type','title_status','model']\n",
    "\n",
    "transformer = make_column_transformer((MEstimateEncoder(), targcats), #MAYBE MENTION OTHER ATTEMPTED TRANSFORMERS IN REPORT\n",
    "                                      (PolynomialFeatures(degree = degree, include_bias = False), ['log_odometer','year','cylinders']))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cars_fix.drop('log_price', axis = 1), cars_fix['log_price'], test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RIDGE REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beginning with Ridge regression\n",
    "pipe = Pipeline([('transformer', transformer),\n",
    "                    ('ridge', Ridge(fit_intercept=True))])\n",
    "\n",
    "param_dict = {'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "grid = GridSearchCV(pipe, param_grid=param_dict)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "train_preds = grid.predict(X_train)\n",
    "test_preds = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the evaluation metrics\n",
    "train_mse = mean_squared_error(y_train, train_preds)\n",
    "test_mse = mean_squared_error(y_test, test_preds)\n",
    "train_mae = mean_absolute_error(y_train, train_preds)\n",
    "test_mae = mean_absolute_error(y_test, test_preds)\n",
    "r2_train = grid.score(X_train, y_train)\n",
    "r2_test = grid.score(X_test, y_test)\n",
    "alpha_val = list(grid.best_params_.values())[0]\n",
    "\n",
    "model_results['Ridge'] = [alpha_val, train_mse, test_mse, train_mae, test_mae, r2_train, r2_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the coefficients derived from the model\n",
    "fit_coef = grid.best_estimator_.named_steps.ridge.coef_\n",
    "fit_feat = grid.best_estimator_.named_steps.transformer.get_feature_names_out()\n",
    "ridge_results = pd.DataFrame(fit_coef).transpose()\n",
    "ridge_results.rename(columns={x:y for x,y in zip(range(0,len(fit_feat)),fit_feat)}, inplace=True)\n",
    "ridge_results.rename({0: 'Ridge'}, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the permutation importance results\n",
    "seqp = Pipeline([('transformer', transformer), ('ridge', Ridge(alpha = alpha_val, fit_intercept=True))])\n",
    "seqp.fit(X_train, y_train)\n",
    "\n",
    "r = permutation_importance(seqp, X_test, y_test,\n",
    "                           n_repeats=30,\n",
    "                           random_state=0)\n",
    "\n",
    "features = cars_fix.drop('log_price', axis=1).columns\n",
    "r_dict = {}\n",
    "\n",
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "    if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "        r_dict[features[i]] = [r.importances_mean[i], r.importances_std[i]]\n",
    "        \n",
    "ridge_perm = pd.DataFrame(r_dict)\n",
    "ridge_perm.rename({0: 'ridge_mean', 1: 'ridge_std'}, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Followed by Lasso regression\n",
    "pipe = Pipeline([('transformer', transformer),\n",
    "                    ('lasso', Lasso(fit_intercept=True))])\n",
    "\n",
    "param_dict = {'lasso__alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "grid = GridSearchCV(pipe, param_grid=param_dict)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "train_preds = grid.predict(X_train)\n",
    "test_preds = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the evaluation metrics\n",
    "train_mse = mean_squared_error(y_train, train_preds)\n",
    "test_mse = mean_squared_error(y_test, test_preds)\n",
    "train_mae = mean_absolute_error(y_train, train_preds)\n",
    "test_mae = mean_absolute_error(y_test, test_preds)\n",
    "r2_train = grid.score(X_train, y_train)\n",
    "r2_test = grid.score(X_test, y_test)\n",
    "alpha_val = list(grid.best_params_.values())[0]\n",
    "\n",
    "model_results['Lasso'] = [alpha_val, train_mse, test_mse, train_mae, test_mae, r2_train, r2_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the coefficients derived from the model\n",
    "fit_coef = grid.best_estimator_.named_steps.lasso.coef_\n",
    "lasso_results = pd.DataFrame(fit_coef).transpose()\n",
    "lasso_results.rename(columns={x:y for x,y in zip(range(0,len(fit_feat)),fit_feat)}, inplace=True)\n",
    "lasso_results.rename({0: 'Lasso'}, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the permutation importance results\n",
    "seqp = Pipeline([('transformer', transformer), ('lasso', Lasso(alpha = alpha_val, fit_intercept=True))])\n",
    "seqp.fit(X_train, y_train)\n",
    "\n",
    "r = permutation_importance(seqp, X_test, y_test,\n",
    "                           n_repeats=30,\n",
    "                           random_state=0)\n",
    "\n",
    "features = cars_fix.drop('log_price', axis=1).columns\n",
    "r_dict = {}\n",
    "\n",
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "    if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "        r_dict[features[i]] = [r.importances_mean[i], r.importances_std[i]]\n",
    "        \n",
    "lasso_perm = pd.DataFrame(r_dict)\n",
    "lasso_perm.rename({0: 'lasso_mean', 1: 'lasso_std'}, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Followed by Lasso regression\n",
    "pipe = Pipeline([('transformer', transformer),\n",
    "                    ('linreg', LinearRegression(fit_intercept=True))])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "train_preds = pipe.predict(X_train)\n",
    "test_preds = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the evaluation metrics\n",
    "train_mse = mean_squared_error(y_train, train_preds)\n",
    "test_mse = mean_squared_error(y_test, test_preds)\n",
    "train_mae = mean_absolute_error(y_train, train_preds)\n",
    "test_mae = mean_absolute_error(y_test, test_preds)\n",
    "r2_train = pipe.score(X_train, y_train)\n",
    "r2_test = pipe.score(X_test, y_test)\n",
    "alpha_val = '-'\n",
    "\n",
    "model_results['Linear'] = [alpha_val, train_mse, test_mse, train_mae, test_mae, r2_train, r2_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the coefficients derived from the model\n",
    "fit_coef = pipe.named_steps.linreg.coef_\n",
    "lin_results = pd.DataFrame(fit_coef).transpose()\n",
    "lin_results.rename(columns={x:y for x,y in zip(range(0,len(fit_feat)),fit_feat)}, inplace=True)\n",
    "lin_results.rename({0: 'Linear'}, axis=0, inplace=True)\n",
    "coef_results = pd.concat([lin_results, ridge_results, lasso_results], axis=0)#, ignore_index=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the permutation importance results\n",
    "seqp = Pipeline([('transformer', transformer), ('ridge', LinearRegression(fit_intercept=True))])\n",
    "seqp.fit(X_train, y_train)\n",
    "\n",
    "r = permutation_importance(seqp, X_test, y_test,\n",
    "                           n_repeats=30,\n",
    "                           random_state=0)\n",
    "\n",
    "features = cars_fix.drop('log_price', axis=1).columns\n",
    "r_dict = {}\n",
    "\n",
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "    if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "        r_dict[features[i]] = [r.importances_mean[i], r.importances_std[i]]\n",
    "        \n",
    "lin_perm = pd.DataFrame(r_dict)\n",
    "lin_perm.rename({0: 'linear_mean', 1: 'linear_std'}, axis=0, inplace=True)\n",
    "perm_results = pd.concat([lin_perm, ridge_perm, lasso_perm], axis=0).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tested parameters:\n",
    "- JamesSteinEncoder, TargetEncoder, and a self-written frequency encoder but MEstimateEncoder gave the best results\n",
    "- Included and excluded bias for polynomial features, but excluding bias gave best results\n",
    "- Cross-validation test-size was tested for 0.2, 0.3, and 0.4 but 0.3 gave the most consistent results across regression models\n",
    "- Polynomial degree was tested between 1 and 10, degree = 4 was selected due to computational constraints and optimal model simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "With some modeling accomplished, we aim to reflect on what we identify as a high-quality model and what we are able to learn from this.  We should review our business objective and explore how well we can provide meaningful insight into drivers of used car prices.  Your goal now is to distill your findings and determine whether the earlier phases need revisitation and adjustment or if you have information of value to bring back to your client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based on the model results, we can derive the following conclusions:**\n",
    "- The results from linear and ridge regression are very difficult to distinguish, these two models perform almost equally as well\n",
    "- The lasso regression model performs worse across all metrics\n",
    "- The lasso and ridge regression models use the same alpha value, so our data is being treated in a comparable manner by both models\n",
    "\n",
    "*MSE & MAE*\n",
    "- The mse and mae values are extremely close for all 3 regression models, although our train values are slightly lower than our test values. Given the size of these values, this discrepancy can be considered to be negligible.\n",
    "\n",
    "*R2 Score*\n",
    "- The R2 scores for our train sets across all 3 regression models are slightly higher than for the test sets, but having a R2 score of around 80% is a very strong result regardless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_results.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based on the permutation importance results, we can derive the following conclusions:**\n",
    "- All 3 regression models showed the greatest influence from year, model, and log_odometer\n",
    "- The lasso regression, which will exclude unnecessary characteristics, eliminated title_status and manufacturer\n",
    "- The lasso results are consistent with the results from the other two models, which demonstrate that manufacturer and title_status contribute minimally to all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*With so many regression coefficients, the results become slightly difficult to interpret, so we will isolate the values for each model and sort them based on their magnitude to identify the largest influences*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coef_results.loc['Linear',:].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear model coefficients:**\n",
    "- For our linear model, the most impactful regressors are log_odometer^2, log_odometer*year, and log_odometer\n",
    "- Log_odometer^2 and log_odometer*year had a positive relationship with changes in the log_price, while log_odometer had a negative correlation\n",
    "- These results are consistent with our intuitive assumptions, and show that the use of polynomial features was important to fully characterize the relationship between these variables and the log_price target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "coef_results.loc['Ridge',:].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge model coefficients:**\n",
    "- For our ridge model, the most impactful regressors are log_odometer^2, log_odometer*year, and log_odometer\n",
    "- Log_odometer^2 and log_odometer*year had a positive relationship with changes in the log_price, while log_odometer had a negative correlation\n",
    "- These results are consistent with our intuitive assumptions, and show that the use of polynomial features was important to fully characterize the relationship between these variables and the log_price target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_results.loc['Lasso',:].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lasso model coefficients:**\n",
    "- For our lasso model, the most impactful regressors are model and transmission\n",
    "- While model was positively correlated with the log_price, transmission was negatively correlated. These correlations relate to the MEstimateEncoder, which uses target values to encode categorical variables. The meaning of these results is that models with greater corresponding prices have a higher correlation with log_prices, a result that does not contribute much to deriving significant conclusions from this regression model\n",
    "- These discrepancies with the ridge and linear models can explain the sub-performance of the lasso regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **We can take the linear and ridge regression models to perform equivalently based on the model, permutation and coefficient results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "Now that we've settled on our models and findings, it is time to deliver the information to the client.  You should organize your work as a basic report that details your primary findings.  Keep in mind that your audience is a group of used car dealers interested in fine-tuning their inventory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSIONS:**\n",
    "1) The majority of data that we have for used cars are sold between 1,000 and 58,0000 USD, and have odometer readings between 1,000 and 400,000 miles\n",
    "2) Most used cars that have been purchased are less than 40 years old\n",
    "3) Diesel and electric cars tend to sell for the highest prices\n",
    "4) California, Florida and Texas have the greatest amount of used car purchases, although state had a very low influence on determining the actual price of the car sold\n",
    "5) The most influential factors in the price of a used car are the odometer readings, year of purchase, and car model\n",
    "6) The number of cylinders has an influence on price when considered together with year of purchase and odometer readings. This can be understood as a perception that more cylinders increase the wear on a car's engine as antiquity and distance traveled are increased\n",
    "7) Some variables have similar influences on the price of a used car, such as fuel + transmission and model + manufacturer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ACTION STEPS:**\n",
    "1) Focus on cars manufactured within the last 40 years\n",
    "2) Lower odometer readings are important for the desirability of a used car\n",
    "3) The model of a car can mitigate higher odometer readings to some extent\n",
    "4) For cars with more cylinders, it is important to focus on those with lower odometer readings in particular"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
